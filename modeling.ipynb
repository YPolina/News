{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Рабочий стол\\News\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, AutoModel, BertTokenizerFast, AdamW, BertModel, AutoModelForSequenceClassification\n",
    "\n",
    "import functionality\n",
    "import importlib\n",
    "importlib.reload(functionality)\n",
    "from functionality import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl = ETL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = etl.loading('./data/etl_data.parquet')\n",
    "preprocessed_data = Data_Preparation(data).data_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = etl.loading('./data/preprocessed_data.parquet')\n",
    "\n",
    "#Data split\n",
    "train_data, test_data = train_test_split(preprocessed_data, test_size=0.20, random_state=42, stratify=preprocessed_data['category_encoded'])\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['category_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTextDataset(train_data['processed_text'], train_data['category_encoded'], tokenizer, max_length=150)\n",
    "val_dataset = CustomTextDataset(val_data['processed_text'], val_data['category_encoded'], tokenizer, max_length=150)\n",
    "test_dataset = CustomTextDataset(test_data['processed_text'], test_data['category_encoded'], tokenizer, max_length=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = sampler(train_data)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    sampler=train_sampler\n",
    ")\n",
    "\n",
    "val_sampler = sampler(val_data)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = 32,\n",
    "    sampler = val_sampler\n",
    ")\n",
    "\n",
    "test_sampler = sampler(test_data)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    sampler=test_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-32\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"yatskopolina1/News\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiNTYzMDJkMi00YThkLTRiYWYtOGU5ZC02MGFiOGEzNjkzYTIifQ==\",\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpointCallback(checkpoint_path=\"model/best_model.pth\"),\n",
    "    EarlyStoppingCallBack(patience=3),\n",
    "    LoggingCallBack(run=run)\n",
    "]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "bert_embeddings_train, train_labels = [], []\n",
    "bert_embeddings_test, test_labels = [], []\n",
    "\n",
    "bert_embeddings_train, train_labels = extract_embeddings(bert_model, train_loader, device)\n",
    "bert_embeddings_test, test_labels = extract_embeddings(bert_model, test_loader, device)\n",
    "\n",
    "X_train = bert_embeddings_train.numpy()\n",
    "y_train = train_labels.numpy()\n",
    "X_test = bert_embeddings_test.numpy()\n",
    "y_test = test_labels.numpy()\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "\n",
    "np.save('./data/X_train.npy', X_train)\n",
    "np.save('./data/y_train.npy', y_train)\n",
    "np.save('./data/X_test.npy', X_test)\n",
    "np.save('./data/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bert_embeddings_train\n",
    "del train_labels\n",
    "del test_labels\n",
    "del bert_embeddings_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/X_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "X_test = np.load('data/X_test.npy')\n",
    "y_test = np.load('data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier\n",
      "F1 score: 0.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = knn.predict(X_test[:100])\n",
    "print(f'KNN Classifier\\nF1 score: {f1_score(y_test[:100], y_pred, average=\"macro\"):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "F1 score: 0.46\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(f'XGBoost Classifier\\nF1 score: {f1_score(y_test, y_pred, average=\"macro\"):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test\n",
    "del X_train\n",
    "del y_test\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Network (RNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [19:25<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0007, Train F1: 0.5103\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [19:08<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3378, Train F1: 0.6229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:36<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4087, Val F1: 0.5913, Val accuracy: 0.5954\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [18:58<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2260, Train F1: 0.6440\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNNClassifier(input_dim =768, hidden_dim=128, output_dim=27)\n",
    "optimizer = optim.AdamW(rnn_model.parameters(), lr=2e-5)\n",
    "\n",
    "rnn_trained = train_model_with_callbacks(\n",
    "    model=rnn_model,\n",
    "    bert_model = bert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    optimizer=optimizer,\n",
    "    criterion = criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    use_bert_embeddings = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1310/1310 [05:45<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Classifier\n",
      "F1 score: 0.59\n",
      "Average Loss: 1.38\n",
      "Accuracy: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "avg_loss, f1, accuracy = eval(rnn_trained, bert_model, test_loader, optimizer, criterion, device, use_bert_embeddings=True)\n",
    "print(f'RNN Classifier\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short-Term Memory (LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [18:48<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9481, Train F1: 0.5400\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [18:53<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3341, Train F1: 0.6285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:39<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4639, Val F1: 0.5814, Val accuracy: 0.5862\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [19:18<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2409, Train F1: 0.6456\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTM(input_dim=768, hidden_dim=128, output_dim=27)\n",
    "optimizer = optim.AdamW(lstm_model.parameters(), lr=2e-5)\n",
    "\n",
    "lstm_trained = train_model_with_callbacks(\n",
    "    model=lstm_model,\n",
    "    bert_model = bert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    optimizer=optimizer,\n",
    "    criterion = criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    use_bert_embeddings = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1310/1310 [06:08<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Classifier\n",
      "F1 score: 0.59\n",
      "Average Loss: 1.42\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "avg_loss, f1, accuracy = eval(lstm_trained, bert_model, test_loader, optimizer, criterion, device, use_bert_embeddings=True)\n",
    "print(f'LSTM Classifier\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/4191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [19:54<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9125, Train F1: 0.5422\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [19:48<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3100, Train F1: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [07:24<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4401, Val F1: 0.5869, Val accuracy: 0.5928\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "att_lstm_model = AttentionLSTM(input_dim=768, hidden_dim=128, output_dim=27)\n",
    "optimizer = optim.AdamW(att_lstm_model.parameters(), lr=2e-5) \n",
    "\n",
    "att_lstm_trained = train_model_with_callbacks(\n",
    "    model=att_lstm_model,\n",
    "    bert_model = bert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    optimizer=optimizer,\n",
    "    criterion = criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    use_bert_embeddings = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1310/1310 [08:57<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Classifier + Attention\n",
      "F1 score: 0.59\n",
      "Average Loss: 1.44\n",
      "Accuracy: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "avg_loss, f1, accuracy = eval(att_lstm_trained, bert_model, test_loader, optimizer, criterion, device, use_bert_embeddings=True)\n",
    "print(f'LSTM Classifier + Attention\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT 2-layers for update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Рабочий стол\\News\\venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [29:43<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5753, Train F1: 0.5488\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [34:20<00:00,  2.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2433, Train F1: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:53<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2771, Val F1: 0.6173\n",
      "Model saved at epoch 2\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [26:03<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1123, Train F1: 0.6655\n"
     ]
    }
   ],
   "source": [
    "bert_classifier = BERT(bert_model)\n",
    "\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr=2e-5)\n",
    "\n",
    "bert_classifier_trained = train_model_with_callbacks(\n",
    "    model=bert_classifier,\n",
    "    bert_model = bert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    optimizer=optimizer,\n",
    "    criterion = criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    use_bert_embeddings = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1310/1310 [08:49<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier\n",
      "F1 score: 0.63\n",
      "Average Loss: 1.29\n",
      "Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "avg_loss, f1, accuracy = eval(bert_classifier_trained, bert_model, test_loader, optimizer, criterion, device, use_bert_embeddings=False)\n",
    "print(f'BERT Classifier\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier\n",
      "F1 score: 0.63\n",
      "Average Loss: 1.28\n",
      "Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "print(f'BERT Classifier\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT 1-layer for update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Рабочий стол\\News\\venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [29:08<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2055, Train F1: 0.6615\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [26:42<00:00,  2.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9989, Train F1: 0.6982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [07:15<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2902, Val F1: 0.6328, Val accuracy: 0.6335\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_classifier = BERT(bert_model)\n",
    "bert_classifier.set_trainable(model.bert.encoder.layer[23], True)\n",
    "\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr=2e-5)\n",
    "\n",
    "bert_classifier_trained = train_model_with_callbacks(\n",
    "    model=bert_classifier,\n",
    "    bert_model = bert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    optimizer=optimizer,\n",
    "    criterion = criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    use_bert_embeddings = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1310/1310 [06:10<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier\n",
      "F1 score: 0.63\n",
      "Average Loss: 1.29\n",
      "Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "avg_loss, f1, accuracy = eval(bert_classifier_trained, bert_model, test_loader, optimizer, criterion, device, use_bert_embeddings=False)\n",
    "print(f'BERT Classifier\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT frozen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [28:25<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.2469, Train F1: 0.0749\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   1%|          | 49/4191 [00:14<19:50,  3.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m bert_classifier \u001b[38;5;241m=\u001b[39m BERT(bert_model)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(bert_classifier\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m bert_classifier_trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_callbacks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bert_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\OneDrive\\Рабочий стол\\News\\functionality.py:927\u001b[0m, in \u001b[0;36mtrain_model_with_callbacks\u001b[1;34m(model, bert_model, train_loader, val_loader, epochs, optimizer, criterion, device, callbacks, use_bert_embeddings)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\User\\OneDrive\\Рабочий стол\\News\\functionality.py:797\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, bert_model, dataloader, optimizer, criterion, device, use_bert_embeddings)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m    796\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(model_outputs, labels)\n\u001b[1;32m--> 797\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    800\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_classifier = BERT(bert_model)\n",
    "\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr=2e-5)\n",
    "\n",
    "bert_classifier_trained = train_model_with_callbacks(\n",
    "    model=bert_classifier,\n",
    "    bert_model = bert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    optimizer=optimizer,\n",
    "    criterion = criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    use_bert_embeddings = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT 1-layer + bigger fine-tunning stack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Рабочий стол\\News\\venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [34:58<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3501, Train F1: 0.3226\n"
     ]
    }
   ],
   "source": [
    "bert_classifier = BERT_bigger(bert_model)\n",
    "bert_classifier.unfreeze(start_layer=11, end_layer=11)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpointCallback(checkpoint_path=\"model/best_model.pth\"),\n",
    "    EarlyStoppingCallBack(patience=3),\n",
    "    LoggingCallBack(run=run)\n",
    "]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr=2e-5)\n",
    "\n",
    "bert_classifier_trained = train_model_with_callbacks(\n",
    "    model=bert_classifier,\n",
    "    bert_model = bert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=1,\n",
    "    optimizer=optimizer,\n",
    "    criterion = criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    use_bert_embeddings = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTUNA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-18 12:54:48,065] A new study created in memory with name: no-name-cbf5a811-2455-4b47-ab76-cc226ca4947b\n",
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-36\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [58:58<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6548, Train F1: 0.2938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:47<00:00,  3.64it/s]\n",
      "[I 2025-01-18 13:58:38,877] Trial 0 finished with value: 1.9342077596269491 and parameters: {'learning_rate': 1.0778278690714714e-06, 'weight_decay': 0.03765380143498119, 'dropout_rate': 0.433869668575926}. Best is trial 0 with value: 1.9342077596269491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-37\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [59:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4761, Train F1: 0.3629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:44<00:00,  3.68it/s]\n",
      "[I 2025-01-18 15:02:50,854] Trial 1 finished with value: 1.703476466181624 and parameters: {'learning_rate': 1.5040812226764134e-06, 'weight_decay': 0.003130782205994873, 'dropout_rate': 0.411590195045326}. Best is trial 1 with value: 1.703476466181624.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-38\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [59:20<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1188, Train F1: 0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [05:16<00:00,  3.31it/s]\n",
      "[I 2025-01-18 16:07:30,384] Trial 2 finished with value: 1.2590044013700867 and parameters: {'learning_rate': 3.6665508640094245e-05, 'weight_decay': 0.00024244866042080518, 'dropout_rate': 0.11166825472311373}. Best is trial 2 with value: 1.2590044013700867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-39\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [59:17<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1758, Train F1: 0.6741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:37<00:00,  3.77it/s]\n",
      "[I 2025-01-18 17:11:29,347] Trial 3 finished with value: 1.270233296597277 and parameters: {'learning_rate': 3.211362714444366e-05, 'weight_decay': 0.021029752068793862, 'dropout_rate': 0.4149551475758383}. Best is trial 2 with value: 1.2590044013700867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-40\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [58:15<00:00,  1.20it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1690, Train F1: 0.6760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [05:18<00:00,  3.29it/s]\n",
      "[I 2025-01-18 18:15:07,016] Trial 4 finished with value: 1.2624036491997825 and parameters: {'learning_rate': 3.057552191742919e-05, 'weight_decay': 0.07971362978433515, 'dropout_rate': 0.41212995838583477}. Best is trial 2 with value: 1.2590044013700867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-41\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [1:00:44<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4062, Train F1: 0.6159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [06:01<00:00,  2.90it/s]\n",
      "[I 2025-01-18 19:21:55,786] Trial 5 finished with value: 1.2456107381873458 and parameters: {'learning_rate': 1.2089835241469762e-05, 'weight_decay': 1.4888046850151236e-05, 'dropout_rate': 0.46104245247032394}. Best is trial 5 with value: 1.2456107381873458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-42\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   1%|          | 33/4191 [00:36<1:14:56,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   1%|          | 34/4191 [45:16<929:21:08, 804.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   1%|          | 36/4191 [45:23<457:06:03, 396.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [1:45:33<00:00,  1.51s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0431, Train F1: 0.4681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:36<00:00,  3.79it/s]\n",
      "[I 2025-01-18 21:12:08,094] Trial 6 finished with value: 1.4315670489241148 and parameters: {'learning_rate': 2.3599931967444015e-06, 'weight_decay': 0.06762422374072875, 'dropout_rate': 0.22689442529323536}. Best is trial 5 with value: 1.2456107381873458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-43\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [57:24<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1159, Train F1: 0.6873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:36<00:00,  3.79it/s]\n",
      "[I 2025-01-18 22:14:12,842] Trial 7 finished with value: 1.3310520941860804 and parameters: {'learning_rate': 6.992891811529071e-05, 'weight_decay': 5.4497477349196754e-05, 'dropout_rate': 0.28831782250404064}. Best is trial 5 with value: 1.2456107381873458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-44\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [57:26<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1362, Train F1: 0.6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:36<00:00,  3.78it/s]\n",
      "[I 2025-01-18 23:16:19,063] Trial 8 finished with value: 1.3484378137433801 and parameters: {'learning_rate': 8.828357528153784e-05, 'weight_decay': 0.0001866218001581195, 'dropout_rate': 0.17996625435672559}. Best is trial 5 with value: 1.2456107381873458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-45\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 4191/4191 [57:28<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2090, Train F1: 0.4320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1048/1048 [04:38<00:00,  3.76it/s]\n",
      "[I 2025-01-19 00:18:28,322] Trial 9 finished with value: 1.5182210913368763 and parameters: {'learning_rate': 1.9225309651194768e-06, 'weight_decay': 3.5292661192855804e-05, 'dropout_rate': 0.230778092498943}. Best is trial 5 with value: 1.2456107381873458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'learning_rate': 1.2089835241469762e-05, 'weight_decay': 1.4888046850151236e-05, 'dropout_rate': 0.46104245247032394}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "study.optimize(lambda trial: optuna_objective(trial, train_loader, val_loader), n_trials=10)\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/yatskopolina1/News/e/NEW-46\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   4%|▎         | 154/4191 [02:06<55:17,  1.22it/s]"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.2089835241469762e-05\n",
    "weight_decay = 1.4888046850151236e-05\n",
    "dropout_rate = 0.46104245247032394\n",
    "EPOCHS = 4\n",
    "BETAS = (0.9, 0.999)\n",
    "EPS = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = BERT(bert_model, num_classes=27, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "bert_identifiers = ['embedding', 'encoder', 'pooler']\n",
    "no_weight_decay_identifiers = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "grouped_model_parameters = [\n",
    "\n",
    "    {'params': [param for name, param in model.named_parameters()\n",
    "                if any(identifier in name for identifier in bert_identifiers) and\n",
    "                not any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
    "        'lr': learning_rate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': weight_decay,\n",
    "        'eps': EPS},\n",
    "\n",
    "    {'params': [param for name, param in model.named_parameters()\n",
    "                if any(identifier in name for identifier in bert_identifiers) and\n",
    "                any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
    "        'lr': learning_rate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': 0.0,\n",
    "        'eps': EPS},\n",
    "\n",
    "    {'params': [param for name, param in model.named_parameters()\n",
    "                if not any(identifier in name for identifier in bert_identifiers)],\n",
    "        'lr': learning_rate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': 0.0,\n",
    "        'eps': EPS}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(grouped_model_parameters)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"yatskopolina1/News\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiNTYzMDJkMi00YThkLTRiYWYtOGU5ZC02MGFiOGEzNjkzYTIifQ==\",\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpointCallback(checkpoint_path=\"model/best_model.pth\"),\n",
    "    EarlyStoppingCallBack(patience=3),\n",
    "    LoggingCallBack(run=run)\n",
    "]\n",
    "\n",
    "trained_model = train_model_with_callbacks(\n",
    "        model=model,\n",
    "        bert_model=bert_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCHS,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        callbacks=callbacks,\n",
    "        use_bert_embeddings=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss, f1, accuracy = eval(trained_model, bert_model, test_loader, optimizer, criterion, device, use_bert_embeddings=False)\n",
    "print(f'BERT Classifier\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 8.828357528153784e-05\n",
    "weight_decay = 0.0001866218001581195\n",
    "dropout_rate = 0.17996625435672559\n",
    "EPOCHS = 4\n",
    "BETAS = (0.9, 0.999)\n",
    "EPS = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = BERT(bert_model, num_classes=27, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "bert_identifiers = ['embedding', 'encoder', 'pooler']\n",
    "no_weight_decay_identifiers = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "grouped_model_parameters = [\n",
    "\n",
    "    {'params': [param for name, param in model.named_parameters()\n",
    "                if any(identifier in name for identifier in bert_identifiers) and\n",
    "                not any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
    "        'lr': learning_rate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': weight_decay,\n",
    "        'eps': EPS},\n",
    "\n",
    "    {'params': [param for name, param in model.named_parameters()\n",
    "                if any(identifier in name for identifier in bert_identifiers) and\n",
    "                any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
    "        'lr': learning_rate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': 0.0,\n",
    "        'eps': EPS},\n",
    "\n",
    "    {'params': [param for name, param in model.named_parameters()\n",
    "                if not any(identifier in name for identifier in bert_identifiers)],\n",
    "        'lr': learning_rate,\n",
    "        'betas': BETAS,\n",
    "        'weight_decay': 0.0,\n",
    "        'eps': EPS}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(grouped_model_parameters)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"yatskopolina1/News\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiNTYzMDJkMi00YThkLTRiYWYtOGU5ZC02MGFiOGEzNjkzYTIifQ==\",\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpointCallback(checkpoint_path=\"model/best_model.pth\"),\n",
    "    EarlyStoppingCallBack(patience=3),\n",
    "    LoggingCallBack(run=run)\n",
    "]\n",
    "\n",
    "trained_model = train_model_with_callbacks(\n",
    "        model=model,\n",
    "        bert_model=bert_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCHS,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        callbacks=callbacks,\n",
    "        use_bert_embeddings=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss, f1, accuracy = eval(trained_model, bert_model, test_loader, optimizer, criterion, device, use_bert_embeddings=False)\n",
    "print(f'BERT Classifier\\nF1 score: {f1:.2f}\\nAverage Loss: {avg_loss:.2f}\\nAccuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
